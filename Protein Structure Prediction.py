# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17s9_BlGynb5iG0B6yr9Qzd9MavQ5dGaL

**Preparing**
"""

!pip install biopython

"""Data **collection**"""

from Bio.PDB import PDBList

# Initialize PDBList
pdbl = PDBList()

# Download a small example PDB file (e.g., 1crn)
# This PDB entry contains both sequence and structural information.
pdbl.retrieve_pdb_file('1crn', file_format='pdb', pdir='.', overwrite=True)

"""Data **Processing**"""

from Bio.PDB import PDBParser, PPBuilder
import numpy as np

# 1. Parse the downloaded PDB file
parser = PDBParser()
structure = parser.get_structure('1crn', 'pdb1crn.ent')

# Extract the protein sequence
ppb = PPBuilder()
for pp in ppb.build_peptides(structure):
    sequence = str(pp.get_sequence())
    print(f"Protein sequence: {sequence}")

# 2. Store extracted sequence and structural data
# The Biopython Structure object is already a good representation.
# We can optionally convert to a DataFrame for specific analyses later if needed.
# For now, we will work with the Structure object directly.

# 3. Identify and handle potential issues (check integrity)
# For this small example, we can simply iterate through residues and atoms
# to confirm their presence. A more robust check would involve looking for gaps
# in residue numbering or missing atoms in standard residues.
print("\nChecking structure integrity:")
for model in structure:
    for chain in model:
        print(f"  Chain ID: {chain.id}")
        for residue in chain:
            print(f"    Residue: {residue.resname} {residue.id[1]}")
            for atom in residue:
                # print(f"      Atom: {atom.name}") # Uncomment to print all atoms
                pass # Just checking if iteration is possible

# 4. Convert structural data to a numerical representation (distance matrix)
# A common representation is the pairwise distance matrix of alpha-carbon atoms.
alpha_carbons = [atom for atom in structure.get_atoms() if atom.name == 'CA']
num_residues = len(alpha_carbons)
distance_matrix = np.zeros((num_residues, num_residues))

for i in range(num_residues):
    for j in range(num_residues):
        distance_matrix[i, j] = alpha_carbons[i] - alpha_carbons[j]

print(f"\nGenerated a {num_residues}x{num_residues} distance matrix.")
# display(pd.DataFrame(distance_matrix).head()) # Uncomment to see a snippet of the matrix

"""Feature **engineering**"""

from collections import Counter
import pandas as pd

# 1. Calculate amino acid composition
amino_acid_composition = Counter(sequence)
print("Amino Acid Composition:")
for aa, count in amino_acid_composition.items():
    print(f"{aa}: {count}")

# 2. Incorporate physicochemical properties
# Using a simple dictionary for hydrophobicity (Kyte & Doolittle scale)
hydrophobicity_scale = {
    'A': 1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C': 2.5,
    'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I': 4.5,
    'L': 3.8, 'K': -3.9, 'M': 1.9, 'F': 2.8, 'P': -1.6,
    'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V': 4.2
}

# Calculate average hydrophobicity of the sequence
total_hydrophobicity = sum(hydrophobicity_scale.get(aa, 0) for aa in sequence)
average_hydrophobicity = total_hydrophobicity / len(sequence) if sequence else 0
print(f"\nAverage Hydrophobicity (Kyte & Doolittle): {average_hydrophobicity:.2f}")

# Create a feature vector for the sequence
# For simplicity, we'll use the amino acid composition and average hydrophobicity
# as features for the entire sequence.
# For more complex models, you would create features for windows or individual residues.
features = {
    'sequence_length': len(sequence),
    'average_hydrophobicity': average_hydrophobicity,
    **amino_acid_composition # Include amino acid counts
}

# Convert to a pandas DataFrame for easier handling
features_df = pd.DataFrame([features])

print("\nExtracted Features:")
display(features_df)

# 3. Evolutionary information is skipped as per instructions.

# 4. Structure the extracted features into a format suitable for input
# The features_df DataFrame is in a suitable format (one row per sequence, columns as features).

"""## Model Selection


"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Reshape

# The input shape needs to be a 2D representation derived from the sequence features.
# Since features_df is currently 1x17, we need to conceptualize how this will be
# transformed into a 2D input related to the sequence length (46).
# A common approach is to create a 2D feature map of size N x N or N x M
# where N is the sequence length. For simplicity and demonstration purposes,
# let's assume we can create a dummy 2D input shape related to sequence length.
# In a real scenario, this would involve creating pairwise features.
sequence_length = features_df['sequence_length'].iloc[0]
# Let's create a dummy 2D input shape, e.g., sequence_length x sequence_length x num_features_per_pair
# Since we only have 17 features for the whole sequence, this is just illustrative.
# A proper implementation would require generating pairwise features.
dummy_input_shape = (sequence_length, sequence_length, 1) # Example: N x N x 1 feature map

input_layer = Input(shape=dummy_input_shape)

# Add 2D convolutional layers
conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(input_layer)
conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(conv1)

# Flatten the output or use further 2D convolutions and then reshape
# Since the output is a 2D matrix, it's better to use convolutions all the way or reshape carefully.
# Let's try to directly predict the N x N matrix using a final convolutional layer
# with a single filter and linear activation for regression.
# The output shape needs to match the distance_matrix shape (sequence_length x sequence_length).
output_layer = Conv2D(1, kernel_size=(3, 3), activation='linear', padding='same')(conv2)

model = Model(inputs=input_layer, outputs=output_layer)

# Compile the model
model.compile(optimizer='adam', loss='mse') # Mean Squared Error for regression

model.summary()

"""Model **Training**"""

import numpy as np

# Prepare input data: Create a dummy 2D input for demonstration.
# In a real scenario, this would involve creating a 2D feature map
# based on pairwise interactions or features derived from the sequence.
# Here, we'll create a dummy input of shape (1, sequence_length, sequence_length, 1)
# filled with some placeholder value, as features_df is not in the correct shape.
# This is NOT how you would typically prepare input for protein structure prediction
# but is done here to make the code runnable given the current features_df format.
sequence_length = features_df['sequence_length'].iloc[0]
dummy_input_data = np.random.rand(1, sequence_length, sequence_length, 1)


# Prepare target data: Reshape the distance_matrix to match the model output shape (batch_size, height, width, channels)
# The model output shape is (None, 46, 46, 1). We have one sample, so the target should be (1, 46, 46, 1).
distance_matrix_target = np.expand_dims(distance_matrix, axis=0) # Add batch dimension
distance_matrix_target = np.expand_dims(distance_matrix_target, axis=-1) # Add channel dimension


# Train the model
# Using small numbers for epochs and batch size for demonstration
epochs = 10
batch_size = 1

print(f"Training model for {epochs} epochs with batch size {batch_size}...")

# Use the dummy input data and the reshaped distance matrix as target
history = model.fit(dummy_input_data, distance_matrix_target, epochs=epochs, batch_size=batch_size, verbose=0)

print("Model training finished.")
# You can access training loss from history.history['loss']
# print("Training Loss per epoch:", history.history['loss'])

"""Model **Evaluation**"""

from sklearn.metrics import mean_squared_error
import numpy as np

# 2. Use the trained model to make a prediction
# Use the dummy input data created during training
predicted_distance_matrix = model.predict(dummy_input_data)

# The prediction shape is (1, 46, 46, 1). Remove batch and channel dimensions
predicted_distance_matrix = np.squeeze(predicted_distance_matrix)

# The target distance matrix is already in a suitable shape (46, 46) after squeezing
# distance_matrix_target shape is (1, 46, 46, 1), squeeze it to (46, 46) for comparison
actual_distance_matrix = np.squeeze(distance_matrix_target)


# 3. Calculate the Mean Squared Error (MSE)
mse = mean_squared_error(actual_distance_matrix.flatten(), predicted_distance_matrix.flatten())


# 4. Calculate the Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)


# 5. Print the calculated MSE and RMSE
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")


# 6. Briefly discuss the limitations of this evaluation
print("\nLimitations of this evaluation:")
print("1. The evaluation is performed on the *same* single sample used for training.")
print("   This does not provide a realistic measure of the model's ability to generalize")
print("   to unseen data, as it is essentially evaluating the model's ability to memorize")
print("   the training example.")
print("2. The training data consisted of a single dummy input sample.")
print("   A real-world protein structure prediction model requires training on a diverse and")
print("   large dataset of protein sequences and structures.")
print("3. The input features used for training were dummy data due to the format of `features_df`.")
print("   Proper feature engineering, creating meaningful 2D representations from the sequence,")
print("   is crucial for training an effective model.")
print("4. Metrics like TM-score or GDT_TS are more commonly used in protein structure prediction")
print("   to assess the overall structural similarity, whereas MSE/RMSE measure distance errors.")
print("   Calculating these metrics requires reconstructing a 3D structure from the predicted")
print("   distance matrix, which is beyond the scope of this basic evaluation step.")

"""Model **Deployment**"""

# 1. Save the trained Keras model to a file in the HDF5 format.
model.save('my_model.keras')
print("Model saved to my_model.keras")

"""**Prediction**"""

import tensorflow as tf
import numpy as np

# 1. Load the saved model
loaded_model = tf.keras.models.load_model('my_model.keras')
print("Model loaded successfully.")

# 2. Define a new protein sequence
# Using the same sequence for demonstration as the model was trained on it (with dummy input)
new_sequence = "TTCCPSIVARSNFNVCRLPGTPEAICATYTGCIIIPGATCPGDYAN"
new_sequence_length = len(new_sequence)

# 3. Prepare the input data for the new sequence
# This step is crucial and would involve proper feature engineering
# to create a 2D input representation (e.g., pairwise features)
# that matches the shape the model was trained on (sequence_length, sequence_length, 1).
# Since we used dummy data for training, we will create dummy data here as well
# for the prediction step to match the expected input shape.
# In a real application, you would calculate meaningful features from the new sequence.
new_dummy_input_data = np.random.rand(1, new_sequence_length, new_sequence_length, 1)
print(f"Prepared dummy input data with shape: {new_dummy_input_data.shape}")

# 4. Use the loaded model's .predict() method
predicted_distance_matrix_new = loaded_model.predict(new_dummy_input_data)

print(f"Prediction successful. Output shape: {predicted_distance_matrix_new.shape}")
# The predicted_distance_matrix_new will have shape (1, sequence_length, sequence_length, 1)
# You can squeeze it to (sequence_length, sequence_length) for easier viewing/use:
predicted_distance_matrix_new = np.squeeze(predicted_distance_matrix_new)
print(f"Squeezed prediction shape: {predicted_distance_matrix_new.shape}")

# You can now work with the predicted_distance_matrix_new, e.g., visualize it or use it for 3D reconstruction.

"""**Visualization**"""

import matplotlib.pyplot as plt
import seaborn as sns

# Create a heatmap of the predicted distance matrix
plt.figure(figsize=(8, 7)) # Adjust figure size for better readability
sns.heatmap(predicted_distance_matrix_new, cmap='viridis')

# Label the axes
plt.xlabel('Residue Index')
plt.ylabel('Residue Index')

# Add a title
plt.title('Predicted Protein Distance Matrix')

# Display the heatmap
plt.show()

"""## Summary:

### Data Analysis Key Findings

*   A protein sequence of 46 amino acids ("TTCCPSIVARSNFNVCRLPGTPEAICATYTGCIIIPGATCPGDYAN") was successfully extracted from the example PDB file '1crn'.
*   A 46x46 distance matrix between alpha-carbon atoms, representing the protein's structure, was successfully generated.
*   Sequence-based features, including amino acid composition and average hydrophobicity (Kyte & Doolittle scale), were successfully extracted.
*   A 2D Convolutional Neural Network (CNN) model was defined, compiled using the Adam optimizer and Mean Squared Error (MSE) loss, and trained on a single sample using dummy input data derived from the sequence length.
*   The model made a prediction on dummy input data for the same sequence, resulting in a predicted 46x46 distance matrix.
*   A basic evaluation using Mean Squared Error (MSE \~68.28) and Root Mean Squared Error (RMSE \~8.26) was performed between the predicted and actual distance matrices for the single training sample.
*   The trained Keras model was successfully saved to a file named `protein_structure_model.h5`.
*   The saved model was successfully loaded and used to make a prediction (a 46x46 distance matrix) for a new sequence (using dummy input data).
*   A heatmap visualization of the predicted distance matrix was successfully generated and displayed.

### Insights or Next Steps

*   The current pipeline demonstrates the conceptual steps but requires significant enhancement in feature engineering to create meaningful 2D input representations from protein sequences for the CNN model.
*   Training and evaluation must be performed on a large, diverse dataset of protein sequences and structures, split into training, validation, and test sets, using appropriate protein structure prediction metrics (like TM-score or GDT\_TS) to assess generalization ability.

Reconstruct 3D structure from predicted distance **matrix**
"""

from sklearn.manifold import MDS
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

# The predicted_distance_matrix_new is already a 46x46 numpy array
# representing the predicted distances.

# Check if the matrix is symmetric (MDS requires a symmetric matrix)
# Due to potential floating point inaccuracies from the model prediction,
# we'll check for approximate symmetry.
if not np.allclose(predicted_distance_matrix_new, predicted_distance_matrix_new.T):
    print("Warning: Predicted distance matrix is not perfectly symmetric. Averaging with transpose.")
    predicted_distance_matrix_new = (predicted_distance_matrix_new + predicted_distance_matrix_new.T) / 2.0

# Check for non-negativity (distances must be non-negative)
if np.any(predicted_distance_matrix_new < 0):
    print("Warning: Predicted distance matrix contains negative values. Setting negative values to zero.")
    predicted_distance_matrix_new[predicted_distance_matrix_new < 0] = 0

# Apply Multidimensional Scaling (MDS) to reconstruct 3D coordinates
# We want to embed the points in 3 dimensions (n_components=3)
# metric=True specifies that we are using a distance matrix
mds = MDS(n_components=3, random_state=0, dissimilarity='precomputed', metric=True)

try:
    # MDS can sometimes fail if the distance matrix is not valid (e.g., doesn't satisfy triangle inequality)
    # or if the number of components is too high for the data's intrinsic dimensionality.
    # In a real scenario, you would need a robust method for distance matrix validation/correction.
    protein_3d_coords = mds.fit_transform(predicted_distance_matrix_new)

    print(f"\nReconstructed 3D coordinates with shape: {protein_3d_coords.shape}")

    # Visualize the reconstructed 3D structure
    fig = plt.figure(figsize=(8, 8))
    ax = fig.add_subplot(111, projection='3d')

    # Plot the alpha-carbon atoms as points
    ax.scatter(protein_3d_coords[:, 0], protein_3d_coords[:, 1], protein_3d_coords[:, 2], c=np.arange(protein_3d_coords.shape[0]), cmap='viridis', s=50)

    # Optionally, connect the points to represent the protein backbone (alpha-carbons)
    ax.plot(protein_3d_coords[:, 0], protein_3d_coords[:, 1], protein_3d_coords[:, 2], color='gray', linestyle='-', linewidth=1)


    ax.set_title('Reconstructed 3D Structure (Alpha-Carbons)')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')

    plt.show()

except Exception as e:
    print(f"\nFailed to reconstruct 3D structure using MDS: {e}")
    print("This can happen if the predicted distance matrix is not a valid metric distance matrix.")
    print("Reconstructing accurate 3D structures from predicted distances is a complex task.")

"""Data Collection: Obtaining a Larger Dataset"""

from Bio.PDB import PDBList
import os

# Initialize PDBList
pdbl = PDBList()

# Define a list of diverse PDB IDs to download
# In a real application, this list would be much larger and carefully selected.
pdb_ids = ['1bgl', '2igd', '3eiy', '4hhb', '5unk'] # Example diverse PDB IDs

# Directory to save the downloaded PDB files
download_dir = './diverse_pdb_files'
os.makedirs(download_dir, exist_ok=True)

print(f"Downloading {len(pdb_ids)} PDB files to '{download_dir}'...")

# Download each PDB file
for pdb_id in pdb_ids:
    try:
        filename = pdbl.retrieve_pdb_file(pdb_id, file_format='pdb', pdir=download_dir, overwrite=True)
        if filename:
            print(f"Downloaded {pdb_id} to {filename}")
        else:
            print(f"Could not download {pdb_id}")
    except Exception as e:
        print(f"Error downloading {pdb_id}: {e}")

print("\nDownload process finished.")
print(f"Check the '{download_dir}' directory for the downloaded files.")

"""Subtask: Process Multiple PDB Files"""

from Bio.PDB import PDBParser, PPBuilder
import os
import numpy as np

# Directory containing the downloaded PDB files
download_dir = './diverse_pdb_files'

# List to store extracted data (sequence and distance matrix for each protein chain)
protein_chain_data = []

# Initialize the PDB parser
parser = PDBParser()
ppb = PPBuilder()

print(f"Processing PDB files in '{download_dir}'...")

# Iterate through all files in the download directory
for filename in os.listdir(download_dir):
    if filename.endswith(".ent"): # PDB files downloaded by Biopython end with .ent
        pdb_path = os.path.join(download_dir, filename)
        pdb_id = os.path.splitext(filename)[0][3:] # Extract PDB ID from filename (e.g., pdb1bgl.ent -> 1bgl)

        print(f"\nProcessing {filename}...")
        try:
            # Parse the PDB file
            structure = parser.get_structure(pdb_id, pdb_path)

            # Iterate through each model and chain in the structure
            for model in structure:
                for chain in model:
                    chain_id = chain.id
                    print(f"  Processing Chain: {chain_id}")

                    # Extract sequence for the current chain
                    # Filter peptides to only include those from the current chain
                    chain_sequences = [str(pp.get_sequence()) for pp in ppb.build_peptides(chain)]

                    # Assuming one sequence per chain for simplicity in this example
                    if chain_sequences:
                        sequence = chain_sequences[0]
                        print(f"    Extracted sequence: {sequence[:50]}...") # Print first 50 chars
                        sequence_length = len(sequence)

                        # Extract alpha-carbon atoms for the current chain
                        alpha_carbons = [atom for atom in chain.get_atoms() if atom.name == 'CA']

                        # Ensure the number of alpha-carbons matches the sequence length (basic check)
                        if len(alpha_carbons) == sequence_length:
                            # Calculate the distance matrix for the chain
                            num_residues = len(alpha_carbons)
                            distance_matrix = np.zeros((num_residues, num_residues))
                            for i in range(num_residues):
                                for j in range(num_residues):
                                    # Use the distance method between Atom objects
                                    distance_matrix[i, j] = alpha_carbons[i] - alpha_carbons[j]


                            # Store the extracted data for the chain
                            protein_chain_data.append({
                                'pdb_id': pdb_id,
                                'chain_id': chain_id,
                                'sequence': sequence,
                                'distance_matrix': distance_matrix,
                                'sequence_length': sequence_length
                            })
                            print(f"    Successfully extracted data for {pdb_id} Chain {chain_id}. Sequence length: {sequence_length}")
                        else:
                            print(f"    Warning: Number of alpha-carbons ({len(alpha_carbons)}) does not match sequence length ({sequence_length}) for {pdb_id} Chain {chain_id}. Skipping chain.")

                    else:
                        print(f"    No protein sequence found in {filename} Chain {chain_id}. Skipping chain.")

        except Exception as e:
            print(f"  Error processing {filename}: {e}. Skipping file.")

print("\nFinished processing PDB files.")
print(f"Successfully processed {len(protein_chain_data)} protein chains.")

# protein_chain_data list now contains dictionaries with data for each successfully processed protein chain.

""" Generate One-Hot Encoded Sequence Features


"""

import numpy as np
import pandas as pd

# Define the standard list of 20 amino acids
amino_acids = 'ACDEFGHIKLMNPQRSTVWY'
aa_to_int = dict((c, i) for i, c in enumerate(amino_acids))

# List to store one-hot encoded sequences and corresponding distance matrices
encoded_data = []

print(f"Generating one-hot encoded features for {len(protein_chain_data)} protein chains...")

for protein_info in protein_chain_data:
    sequence = protein_info['sequence']
    distance_matrix = protein_info['distance_matrix']
    pdb_id = protein_info['pdb_id']
    chain_id = protein_info['chain_id']
    sequence_length = protein_info['sequence_length']


    # Create one-hot encoding for the sequence
    # Initialize with zeros (sequence_length x num_amino_acids)
    one_hot_encoded_sequence = np.zeros((sequence_length, len(amino_acids)), dtype=int)

    # Populate the one-hot encoded matrix
    for i, aa in enumerate(sequence):
        if aa in aa_to_int:
            int_value = aa_to_int[aa]
            one_hot_encoded_sequence[i, int_value] = 1
        else:
            # Handle non-standard amino acids or other characters if necessary
            # For now, we'll print a warning and keep the row as all zeros
            print(f"  Warning: Non-standard amino acid '{aa}' found in {pdb_id} Chain {chain_id} at position {i}. Skipping encoding for this residue.")
            # Optionally, you could add a column for 'unknown' amino acids

    # Store the one-hot encoded sequence and its corresponding distance matrix
    # We'll store them as a tuple or dictionary for later use
    encoded_data.append({
        'pdb_id': pdb_id,
        'chain_id': chain_id,
        'one_hot_sequence': one_hot_encoded_sequence,
        'distance_matrix': distance_matrix,
        'sequence_length': sequence_length
    })
    print(f"  Processed {pdb_id} Chain {chain_id}. Sequence length: {sequence_length}. One-hot shape: {one_hot_encoded_sequence.shape}")


print("\nFinished generating one-hot encoded features.")
print(f"Successfully encoded data for {len(encoded_data)} protein chains.")

# encoded_data list now contains dictionaries with one-hot encoded sequences and distance matrices.
# The shape of one_hot_sequence for a protein of length N will be (N, 20).
# The shape of distance_matrix for a protein of length N will be (N, N).

"""Prepare Data for Model Training"""

# Extract and display sequence lengths from encoded_data
sequence_lengths = [item['sequence_length'] for item in encoded_data]

print("Sequence lengths of processed protein chains:")
print(sequence_lengths)

# You can also get some statistics
import numpy as np
print(f"\nMinimum length: {np.min(sequence_lengths)}")
print(f"Maximum length: {np.max(sequence_lengths)}")
print(f"Average length: {np.mean(sequence_lengths):.2f}")
print(f"Median length: {np.median(sequence_lengths)}")

"""Subtask: Pad/Truncate Data and Split into Train/Test Sets

Reasoning: Pad or truncate the one-hot encoded sequences and distance matrices to a fixed maximum length, structure them into input (X) and target (y) arrays, and split the data into training and testing sets.
"""

from sklearn.model_selection import train_test_split
import numpy as np

# Define the chosen maximum sequence length
max_sequence_length = 120
num_amino_acids = 20 # We used 20 standard amino acids for one-hot encoding

# Lists to store padded/truncated inputs and targets
padded_inputs = []
padded_targets = []

print(f"Padding/truncating data to a maximum length of {max_sequence_length}...")

for protein_info in encoded_data:
    one_hot_sequence = protein_info['one_hot_sequence']
    distance_matrix = protein_info['distance_matrix']
    sequence_length = protein_info['sequence_length']

    # --- Handle Input Features (One-Hot Encoded Sequence) ---
    # Pad or truncate the one-hot encoded sequence
    if sequence_length > max_sequence_length:
        # Truncate if longer
        padded_sequence = one_hot_sequence[:max_sequence_length, :]
        # print(f"  Truncating sequence from {sequence_length} to {max_sequence_length}")
    else:
        # Pad if shorter
        padding_needed = max_sequence_length - sequence_length
        # Create a padding array of zeros
        padding_array = np.zeros((padding_needed, num_amino_acids), dtype=int)
        # Concatenate the original sequence with the padding array
        padded_sequence = np.vstack((one_hot_sequence, padding_array))
        # print(f"  Padding sequence from {sequence_length} to {max_sequence_length}")

    # Reshape the padded sequence to match the expected 2D CNN input format if necessary.
    # NOTE: As discussed before, the ideal input for an NxN distance matrix
    # prediction with a 2D CNN is an NxN feature map derived from the sequence.
    # For simplicity here, we will reshape the (max_sequence_length, 20) padded
    # one-hot encoding into a dummy 2D input shape (max_sequence_length, max_sequence_length, 1)
    # This is a placeholder and needs proper feature engineering for a real model.
    # A very basic approach could be to tile the one-hot encoding, but that's not
    # biologically meaningful. Let's just create a dummy input for now to make the code runnable.
    # A better approach would be to compute pairwise features from the padded_sequence.

    # Creating a dummy 2D input (placeholder - needs proper feature engineering)
    # For simplicity, we'll create a dummy 2D input of shape (max_sequence_length, max_sequence_length, 1)
    # filled with some value, as the one-hot encoding itself is not directly this shape.
    # In a real scenario, you would compute pairwise features here from 'padded_sequence'.
    dummy_2d_input = np.random.rand(max_sequence_length, max_sequence_length, 1) # Placeholder

    padded_inputs.append(dummy_2d_input)


    # --- Handle Target Labels (Distance Matrix) ---
    # Pad or truncate the distance matrix
    if sequence_length > max_sequence_length:
        # Truncate if longer (take the top-left block)
        padded_matrix = distance_matrix[:max_sequence_length, :max_sequence_length]
        # print(f"  Truncating matrix from ({sequence_length}, {sequence_length}) to ({max_sequence_length}, {max_sequence_length})")
    else:
        # Pad if shorter
        padding_needed = max_sequence_length - sequence_length
        # Create a padding matrix of zeros
        padding_matrix = np.zeros((max_sequence_length, max_sequence_length)) # Pad to NxN
        # Copy the original matrix into the top-left corner of the padding matrix
        padding_matrix[:sequence_length, :sequence_length] = distance_matrix
        padded_matrix = padding_matrix
        # print(f"  Padding matrix from ({sequence_length}, {sequence_length}) to ({max_sequence_length}, {max_sequence_length})")

    # Ensure the target has a channel dimension for the CNN output (batch_size, height, width, channels)
    padded_matrix = np.expand_dims(padded_matrix, axis=-1) # Add channel dimension

    padded_targets.append(padded_matrix)


print("Padding/truncation finished.")

# Convert lists to NumPy arrays
X = np.array(padded_inputs)
y = np.array(padded_targets)

print(f"\nPrepared input data shape (X): {X.shape}")
print(f"Prepared target data shape (y): {y.shape}")

# Split data into training and testing sets
# Using a standard split ratio, e.g., 80% train, 20% test
test_size = 0.2
random_state = 42 # for reproducibility

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)

print(f"\nData split into training and testing sets:")
print(f"Training input shape (X_train): {X_train.shape}")
print(f"Training target shape (y_train): {y_train.shape}")
print(f"Testing input shape (X_test): {X_test.shape}")
print(f"Testing target shape (y_test): {y_test.shape}")

"""Model Selection and Compilation"""

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Reshape

# The input shape needs to match the padded/truncated input data shape (max_sequence_length, max_sequence_length, 1).
input_shape = (max_sequence_length, max_sequence_length, 1)

input_layer = Input(shape=input_shape)

# Add 2D convolutional layers
conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(input_layer)
conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(conv1)

# Add more convolutional layers or adjust kernel sizes as needed for complexity
# conv3 = Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same')(conv2)

# The output shape needs to match the padded/truncated target distance matrix shape (max_sequence_length, max_sequence_length, 1).
output_layer = Conv2D(1, kernel_size=(3, 3), activation='linear', padding='same')(conv2) # Using linear activation for regression


model = Model(inputs=input_layer, outputs=output_layer)

# Compile the model
model.compile(optimizer='adam', loss='mse') # Mean Squared Error for regression

model.summary()

"""Train the Model"""

# Train the model
# Using small numbers for epochs and batch size for demonstration
epochs = 50 # Increased epochs slightly for more training
batch_size = 4 # Using a slightly larger batch size

print(f"Training model for {epochs} epochs with batch size {batch_size}...")

# Use the prepared training data (X_train, y_train)
history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)

print("Model training finished.")

# You can access training loss per epoch
print("Training Loss per epoch:", history.history['loss'])

"""Model Evaluation"""

from sklearn.metrics import mean_squared_error
import numpy as np

print("Evaluating model on the test set...")

# Use the trained model to make predictions on the test set
predicted_distance_matrices_test = model.predict(X_test)

# The predictions will have shape (num_test_samples, max_sequence_length, max_sequence_length, 1)
# The actual target y_test has the same shape.

# Calculate Mean Squared Error (MSE) and Root Mean Squared Error (RMSE) for the test set
# We can flatten the matrices to calculate overall MSE/RMSE across all test samples and all positions.
# Alternatively, we could calculate it per sample and average. Let's flatten for a single overall metric.
mse_test = mean_squared_error(y_test.flatten(), predicted_distance_matrices_test.flatten())
rmse_test = np.sqrt(mse_test)

print(f"\nEvaluation Results on Test Set:")
print(f"Mean Squared Error (MSE): {mse_test:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse_test:.4f}")

# Discuss limitations
print("\nImportant Considerations:")
print("1. The evaluation is based on a very small dataset of proteins.")
print("   Training on a much larger and more diverse dataset is essential for a robust model.")
print("2. The input features (X_test) are currently dummy data due to simplified feature engineering.")
print("   Meaningful 2D features derived from the sequence are required for effective prediction.")
print("3. MSE/RMSE on distance matrices is an indirect measure of structural accuracy.")
print("   Metrics like TM-score or RMSD on reconstructed 3D structures are more standard.")
print("   Reconstructing 3D structures from predicted distance matrices is a complex task itself.")

"""## Prediction and Visualization

 Reconstruct 3D structure from predicted distance matrix


"""

import tensorflow as tf
import numpy as np
from sklearn.manifold import MDS
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Define the new protein sequence provided by the user (Human Keratine Protein FASTA Format)
new_sequence = "VTLARTDLEMQIEGLKEELAYLRKNHEEEMLALRGQTGGDVNVEMDAAPGVDLSRILNEMRDQYEQMAEKNRRDAETWFLSKTEELNKEVASNSELVQSSRSEVTELRRVLQGLEIELQSQLSTKASLENSLEETKGRYCMQLSQIQGLIGSVEEQLAQLRCEMEQQSQEYQILLDVKTRLEHEIATYRRLLXGEDAHLSSQQASGQSYS SSREVFTSSSSSSSRQTRPILKEQSSSSFSQGQSS"
new_sequence_length = len(new_sequence)

# Prepare the input data for the new sequence
# This step involves creating the same type of 2D input feature map
# that the model was trained on (padded and in the dummy 2D format).
# In a real application, you would calculate meaningful features from the new sequence.

# Determine the size after padding/truncation for generating dummy input
# We need max_sequence_length from the data preparation step (cell a30f2aef)
# Using the value from the kernel state if available, otherwise default.
max_sequence_length = 120 # Ensure max_sequence_length is defined

effective_sequence_length = min(new_sequence_length, max_sequence_length)

# Create dummy 2D input data with the expected shape
# The shape needs to be (1, max_sequence_length, max_sequence_length, 1) for prediction batching.
new_dummy_input_data = np.random.rand(1, max_sequence_length, max_sequence_length, 1)
print(f"Prepared dummy input data with shape: {new_dummy_input_data.shape}")


# Use the trained model to predict the distance matrix for the new sequence
# Ensure the model object is available from previous steps (it should be after running the training cell)
# Check if the model object exists, otherwise regenerate it (less ideal but safer)
if 'model' not in locals():
    print("Model not found in current environment. Redefining and compiling model...")
    import tensorflow as tf
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Input, Conv2D

    input_shape = (max_sequence_length, max_sequence_length, 1)
    input_layer = Input(shape=input_shape)
    conv1 = Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(input_layer)
    conv2 = Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(conv1)
    output_layer = Conv2D(1, kernel_size=(3, 3), activation='linear', padding='same')(conv2)
    model = Model(inputs=input_layer, outputs=output_layer)
    model.compile(optimizer='adam', loss='mse')
    print("Model redefined and compiled.")


predicted_distance_matrix_new = model.predict(new_dummy_input_data)

# The prediction shape is (1, max_sequence_length, max_sequence_length, 1). Remove batch and channel dimensions
predicted_distance_matrix_new = np.squeeze(predicted_distance_matrix_new)

print(f"Prediction successful. Output shape: {predicted_distance_matrix_new.shape}")


# --- Attempt 3D Reconstruction using MDS ---
# We need a distance matrix of size N x N where N is the *actual* length of the sequence
# we are interested in visualizing, not the padded length.
# However, our prediction is on the padded size (max_sequence_length x max_sequence_length).
# We need to extract the relevant part of the predicted matrix corresponding to the actual sequence length.

# Extract the part of the predicted matrix corresponding to the actual sequence length
# This assumes the model's prediction in the padded region is meaningful for the
# non-padded part, which is an assumption due to the dummy input and padding.
predicted_distance_matrix_for_mds = predicted_distance_matrix_new[:new_sequence_length, :new_sequence_length]

# Check if the matrix is symmetric and non-negative for MDS
if not np.allclose(predicted_distance_matrix_for_mds, predicted_distance_matrix_for_mds.T):
    print("Warning: Predicted distance matrix for MDS is not perfectly symmetric. Averaging with transpose.")
    predicted_distance_matrix_for_mds = (predicted_distance_matrix_for_mds + predicted_distance_matrix_for_mds.T) / 2.0

if np.any(predicted_distance_matrix_for_mds < 0):
    print("Warning: Predicted distance matrix for MDS contains negative values. Setting negative values to zero.")
    predicted_distance_matrix_for_mds[predicted_distance_matrix_for_mds < 0] = 0


# Apply Multidimensional Scaling (MDS)
mds = MDS(n_components=3, random_state=0, dissimilarity='precomputed', metric=True)

try:
    protein_3d_coords = mds.fit_transform(predicted_distance_matrix_for_mds)

    print(f"\nReconstructed 3D coordinates with shape: {protein_3d_coords.shape}")

    # Visualize the reconstructed 3D structure
    fig = plt.figure(figsize=(8, 8))
    ax = fig.add_subplot(111, projection='3d')

    # Plot the alpha-carbon atoms as points
    ax.scatter(protein_3d_coords[:, 0], protein_3d_coords[:, 1], protein_3d_coords[:, 2], c=np.arange(protein_3d_coords.shape[0]), cmap='viridis', s=50)

    # Optionally, connect the points to represent the protein backbone (alpha-carbons)
    ax.plot(protein_3d_coords[:, 0], protein_3d_coords[:, 1], protein_3d_coords[:, 2], color='gray', linestyle='-', linewidth=1)


    ax.set_title('Reconstructed 3D Structure from Predicted Distance Matrix')
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')

    plt.show()

except Exception as e:
    print(f"\nFailed to reconstruct 3D structure using MDS: {e}")
    print("This can happen if the predicted distance matrix is not a valid metric distance matrix for the actual sequence length.")
    print("Reconstructing accurate 3D structures from predicted distances is a complex task.")